Project 1 Report: Anthony Devesa (6224283), Parshatd Govindasamy (6378670),  Rahuul Rangaraj(6360064)

Exercise 1: Semaphores: The first exercise was basically to understand and use semaphores in NACHOS to synchronize threads. We modified the threadtest.cc code to demonstrate the effect of unsynchronized access to a shared variable and then to use semaphores synchronization primitives in NACHOS to achieve proper synchronization. For the implementation we modified the code to use semaphore operations (provided in synch.cc) to enforce mutual exclusion and to implement a simple barrier, ensuring that all threads finish updating the shared variable before printing the final value. Originally, when the threads version of Nachos starts, it creates a single thread that begins executing the ThreadTest() function in threadtest.cc. The older ThreadTest() forked one new thread, and both the new thread and the original thread executed the function SimpleThread(). In our modified version, we change the prototype of ThreadTest() to take an integer argument n and fork n new threads. We then modified SimpleThread() so that each thread runs a loop 5 times. For each iteration, the thread reads the shared variable, prints its value, yields, increments the variable, yields again, and repeats. After the loop, each thread prints the final value of the shared variable. Without synchronization, race conditions occur, and the output is unpredictable. With proper synchronization, every iteration should increment the variable by exactly one, and every thread should see the same final value. We had some initial errors regarding the absence of a multilib package on Linux. We resolved those issues by installing the necessary packages. With semaphores, the output showed a strictly increasing sequence and the final value matched the expected total increments (5 iterations × number of threads). We defined our synchronization changes under a custom macro (HW1_SEMAPHORES). When compiling with -DHW1_SEMAPHORES, our synchronized version is compiled. In synch.cc, a binary semaphore is globally defined and implemented with P() and V() operations. This semaphore is used to protect access to the shared variable. In our threadtest.cc, we implemented the following function (compiled under #ifdef HW1_SEMAPHORES). The mutex->P() and mutex->V() calls ensure that only one thread at a time reads, prints, and increments SharedVariable. This prevents race conditions. For Barrier implementation, after the loop, each thread safely decrements numThreadsActive (protected by the semaphore) and then enters a loop yielding until all threads have finished. This ensures that no thread prints the final value until everyone has completed updating. We modified ThreadTest(int n) so that it forks n-1 threads and sets numThreadsActive to n. The main thread runs as thread 0. This allows us to test for various values of n and observe the correct final shared variable value.

2. Exercise2: Locks: In this exercise, we replaced the semaphore-based synchronization used in Exercise 1 with lock-based synchronization. The goal was to ensure mutual exclusion by implementing Lock operations so that only the thread that holds the lock can release it. 
We implemented the Lock functions in synch.cc (wrapped in #ifdef HW1_LOCKS). For the implementation, we declared a global lock and a shared variable, along with a counter (numThreadsActive) that tracks how many threads are still running. The lock is used to protect access to SharedVariable as well as the counter, ensuring that only one thread at a time can modify these values. In our synchronized version of SimpleThread(), each thread loops five times. In each iteration, the thread does the following: Acquires the Lock: Before reading or updating the shared variable, the thread calls lock->Acquire(), ensuring that it has exclusive access. Critical Section: Inside the critical section, it reads the current value of SharedVariable, prints message, increments the variable by 1, and then releases the lock with lock->Release(). After releasing the lock, the thread yields the CPU with currentThread->Yield() so that other threads have a chance to run. For Barrier Synchronization, after the loop, the thread acquires the lock again, decrements the numThreadsActive counter to signal that it has finished, and releases the lock. Then, it waits (by continuously yielding) until numThreadsActive becomes zero. This barrier ensures that no thread prints the final value until all threads have finished updating the shared variable. Once the barrier condition is met, the thread prints the final value of SharedVariable. There were some linking and visibility errors (e.g., undefined references) which we resolved by ensuring that the proper macros (like -DHW1_LOCKS) were defined in the Makefile. We learned to check thread ownership (using ASSERT(isHeldByCurrentThread())) to prevent errors.

3. Exercise 3: Condition Variables: In Exercise 3, we were tasked with implementing the missing condition variable operations in Nachos. We had to implement the following functions in synch.cc (and declare them in synch.h). Condition variables allow threads to wait for specific events to occur while releasing the associated lock and then reacquiring it upon wakeup. We added implementations for: Condition::Condition(char debugName):* This constructor initializes a condition object. The provided debug name is stored for debugging purposes. Condition::~Condition(): This destructor deallocates the condition object. Condition::Wait(Lock conditionLock):* This function causes the current thread to wait on the condition. It must be called while the lock is held. The implementation releases the lock, blocks the thread, and then re-acquires the lock once the thread is signaled. Condition::Signal(Lock conditionLock):* This function wakes up one waiting thread. Condition::Broadcast(Lock conditionLock):* This function wakes up all threads waiting on the condition. In synch.h, we added the prototypes for the condition variable operations. (The original file only had an interface; our implementation is wrapped in #ifdef HW1_CONDITIONS.) In synch.cc, we implemented the condition variable functions. For example, the Wait function creates a temporary semaphore (initialized to 0), appends it to the condition’s waiting queue, releases the associated lock, waits on the semaphore (with P()), and finally re-acquires the lock. We also added debugging prints in the Wait function (e.g., printing whether the lock is held when Wait() is called) to verify that the calling thread indeed holds the lock before waiting. This satisfies the assertion within the condition variable implementation. Issues Encountered: We had an assertion failure in Condition::Wait() when the lock was not held by the calling thread. This turned out to be because we were not compiling the lock-based implementation (HW1_LOCKS) correctly. By ensuring that -DHW1_LOCKS was defined along with our condition code, we fixed the issue. We wrote a test branch (wrapped in #ifdef HW1_CONDITIONS) in threadtest.cc to verify our condition variable implementation. In this test: A global lock (condLock) and a condition variable (barrierCV) are created. Each thread (in SimpleThread()) increments a shared variable under protection of the lock. After the loop, each thread decrements a global counter (numThreadsActive). If it is the last thread (i.e., numThreadsActive reaches 0), it calls Broadcast() on the condition variable so that all waiting threads are woken up. Otherwise, a thread calls Wait() on the condition variable. After waking, each thread prints the final value of the shared variable.
4. Exercise 4: Elevator Simulation: The final exercise was to build an elevator controller simulation where, the elevator is represented as a thread, each student/faculty (person) is represented as a thread and the elevator should handle multiple requests simultaneously (up to 5 people at a time) and print specific messages. The elevator should simulate travel delays (50 ticks per floor) and eventually terminate after serving all requests. Files Involved: elevator.h file: Declares the ELEVATOR class, the Person struct, and the interface functions (Elevator(), ArrivingGoingFromTo(), and ElevatorTest()). elevator.cc file: Implements the elevator thread logic. Constructor: Initializes floors, occupancy, locks, condition variables, and termination counters (totalRequests and servedRequests). start(): Contains an infinite loop where the elevator prints its current floor, signals waiting threads (both for entering and leaving), simulates travel delay, and moves to the next floor. A termination condition is checked: if servedRequests equals totalRequests, the loop breaks. hailElevator(): Called by person threads to request the elevator. It waits until the elevator arrives at the person’s floor, allows them to board (if there is capacity), then waits until the destination floor is reached, and finally lets them exit while incrementing servedRequests. ArrivingGoingFromTo(): Creates a Person struct, prints the request message, and forks a new person thread. ElevatorTest.cc: Creates the elevator thread and spawn’s multiple person threads. Importantly, it sets total requests in the elevator instance so that the termination condition in start() can be met. main.cc: Calls ElevatorTest(5, 25) under the HW1_ELEVATOR directive so that the elevator simulation starts when that macro is defined. We did some code changes for termination by adding termination counters (totalRequests and servedRequests) in the ELEVATOR class.  Once all requests are served, the elevator thread exits the loop and prints a termination message. We encountered linking errors (e.g., undefined reference to ElevatorTest) because the source file containing its definition wasn’t included in the Makefile. We fixed this by adding ElevatorTest.cc and elevator.cc to the THREAD_C list in Makefile.common. Infinite Loop: The elevator was running indefinitely because the termination condition was not met. After ensuring totalRequests is set correctly and enough person threads are generated, the simulation terminates when all requests have been served. 

